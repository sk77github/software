块存储，文件存储，共享存储



【块存储】
典型设备：磁盘阵列，硬盘

块存储主要是将裸磁盘空间整个映射给主机使用的，可以通过划逻辑盘、做Raid、或者LVM（逻辑卷）等种种方式逻辑划分出N个逻辑的硬盘。
操作系统还需要对挂载的裸硬盘进行分区、格式化后，才能使用，与平常主机内置硬盘的方式完全无异。
优点：
1、  这种方式的好处当然是因为通过了Raid与LVM等手段，对数据提供了保护。
2、  另外也可以将多块廉价的硬盘组合起来，成为一个大容量的逻辑盘对外提供服务，提高了容量。
3、  写入数据的时候，由于是多块磁盘组合出来的逻辑盘，所以几块磁盘可以并行写入的，提升了读写效率。
4、  很多时候块存储采用SAN架构组网，传输速率以及封装协议的原因，使得传输速度与读写速率得到提升。
缺点：1、采用SAN架构组网时，需要额外为主机购买光纤通道卡，还要买光纤交换机，造价成本高。
2、主机之间的数据无法共享，在服务器不做集群的情况下，块存储裸盘映射给主机，再格式化使用后，对于主机来说相当于本地盘，
那么主机A的本地盘根本不能给主机B去使用，无法共享数据。
3、不利于不同操作系统主机间的数据共享

【文件存储】
典型设备：FTP、NFS服务器

为了克服上述文件无法共享的问题，所以有了文件存储。       
文件存储也有软硬一体化的设备，但是其实普通拿一台服务器/笔记本，只要装上合适的操作系统与软件，就可以架设FTP与NFS服务了，架上该类服务之后的服务器，
就是文件存储的一种了。主机A可以直接对文件存储进行文件的上传下载，与块存储不同，主机A是不需要再对文件存储进行格式化的，因为文件管理功能已经由文件存储
自己搞定了。

优点：
1、造价交低：随便一台机器就可以了，另外普通以太网就可以，根本不需要专用的SAN网络，所以造价低。
2、方便文件共享：例如主机A（WIN7，NTFS文件系统），主机B（Linux，EXT4文件系统），想互拷一部电影，本来不行。加了个主机C（NFS服务器），
然后可以先A拷到C，再C拷到B就OK了。
缺点：
读写速率低，传输速率慢：以太网，上传下载速度较慢，另外所有读写都要1台服务器里面的硬盘来承担，相比起磁盘阵列动不动就几十上百块硬盘同时读写，
速率慢了许多。

【对象存储】
典型设备：内置大容量硬盘的分布式服务器

对象存储最常用的方案，就是多台服务器内置大容量硬盘，再装上对象存储软件，然后再额外搞几台服务作为管理节点，安装上对象存储管理软件。
管理节点可以管理其他服务器对外提供读写访问功能。       

之所以出现了对象存储这种东西，是为了克服块存储与文件存储各自的缺点，发扬它俩各自的优点。简单来说块存储读写快，不利于共享，文件存储读写慢，利于共享。
能否弄一个读写快，利于共享的出来呢。于是就有了对象存储。       

为什么对象存储兼具块存储与文件存储的好处，还要使用块存储或文件存储呢？
1、有一类应用是需要存储直接裸盘映射的，例如数据库。因为数据库需要存储裸盘映射给自己后，再根据自己的数据库文件系统来对裸盘进行格式化的，
所以是不能够采用其他已经被格式化为某种文件系统的存储的。此类应用更适合使用块存储。
2、对象存储的成本比起普通的文件存储还是较高，需要购买专门的对象存储软件以及大容量硬盘。如果对数据量要求不是海量，只是为了做文件共享的时候，
直接用文件存储的形式好了，性价比高。


-----------------------------------------------------------------------------------------------
数据库系统:
数据库软件：由底层文件系统和上层表格系统构成
运行数据库软件的数据库服务器硬件
保存数据库数据的数据库存储硬件(即共享存储)




数据库软件的维护升级依然有很大的风险:
2013年6月中国工商银行系统不可用即是其数据库DB2的维护升级导致:
2014年8月美国国务院签证数据库系统超过1周的不可用也是其数据库Oracle的维护升级导致。



http://blog.sina.com.cn/s/blog_3fc85e260102vhh2.html
基本假设就是硬件(服务器、存储、网络等)是不可靠的，
另一个基本假设是单机(数据库服务器及其存储)无法满足互联网业务的需求。
因此，要有一个多机(分布式)系统，并且必须保证任何时刻出现的少量硬件(服务器、存储、网络等)异常不影响业务。

Paxos协议，每一笔事务，主库执行完成后，要同步到半数以上




主备镜像，灾备：
主备数据的数据一致性如何做？


------------------------------------------------------------------------------------------------------------------------

很多人对RAID5的性能有些质疑，为什么redo不能放在RAID5上，因为IO比较小，如果不能写满整个条带，需要把条带中的数据先读出来，
然后算好校验再写回去，所以性能比RAID10差很多。而对于吞吐类型的顺序读写，RAID5的性能很好，不比RAID10差。 



RAID基础，RAID10与RAID01比较，RAID10与RAID5比较

转自：http://btxigua.itpub.net/post/34419/406437

文档中，RAID10与RAID5抄袭了piner的文章，敬请谅解。

一、RAID10和RAID01的比较
 RAID10是先做镜象，然后再做条带。
 RAID01则是先做条带，然后再做镜象。
比如以6个盘为例，RAID10就是先将盘分成3组镜象，然后再对这3个RAID1做条带。RAID01则是先利用3块盘做RAID0，然后将另外3块盘做为RAID0的镜象。
下面以4块盘为例来介绍安全性方面的差别：
1、RAID10的情况
这种情况中，我们假设当DISK0损坏时，在剩下的3块盘中，只有当DISK1一个盘发生故障时，才会导致整个RAID失效，我们可简单计算故障率为1/3。
2、RAID01的情况
这种情况下，我们仍然假设DISK0损坏，这时左边的条带将无法读取（raid0是一个整体，当一块磁盘发生故障时，整个raid0将无法读取）。在剩下的3块盘中，只要DISK2，DISK3两个盘中任何一个损坏，都会导致整个RAID失效，我们可简单计算故障率为2/3。
因此RAID10比RAID01在安全性方面要强。

从数据存储的逻辑位置来看，在正常的情况下RAID01和RAID10是完全一样的，而且每一个读写操作所产生的IO数量也是一样的，所以在读写性能上两者没什么区别。而当有磁盘出现故障时，比如前面假设的DISK0损坏时，我们也可以发现，这两种情况下，在读的性能上面也将不同，RAID10的读性能将优于RAID01。

BTxigua 上传了这个图片:


二、RAID10和RAID5的比较
为了方便对比，这里拿同样多驱动器的磁盘来做对比，RAID5选择3D+1P的RAID方案，RAID10选择2D+2D的RAID方案。

1、安全性方面的比较

其实在安全性方面，勿须质疑，肯定是RAID10的安全性高于RAID5。我们也可以从简单的分析来得出。当盘1损坏时，对于RAID10，只有当盘1对应的镜象盘损坏，才导致RAID失效。但是对于RAID5，剩下的3块盘中，任何一块盘故障，都将导致RAID失效。

在恢复的时候，RAID10恢复的速度也快于RAID5。

2、空间利用率的比较

RAID10的利用率是50%，RAID5的利用率是75%。硬盘数量越多，RAID5的空间利用率越高。

3、读写性能方面的比较

主要分析分析如下三个过程：读，连续写，离散写。

在介绍这三个过程之前，先介绍一个特别重要的概念：cache。

cache已经是整个存储的核心所在，就是中低端存储，也有很大的cache存在，包括最简单的raid卡，一般都包含有几十，甚至几百兆的raid cache。

cache的主要作用是什么呢？体现在读与写两个不同的方面，如果作为写，一般存储阵列只要求写到cache就算完成了写操作，所以，阵列的写是非常快速的，在写cache的数据积累到一定程度，阵列才把数据刷到磁盘，可以实现批量的写入，至于cache数据的保护，一般都依赖于镜相与电池（或者是UPS）。

cache的读一样不可忽视，因为如果读能在cache中命中的话，将减少磁盘的寻道，因为磁盘从寻道开始到找到数据，一般都在6ms以上，而这个时间，对于那些密集型io的应用可能不是太理想。但是，如果cache能命中，一般响应时间则可以在1ms以内。两者应该相差3个数量级（1000倍）。

1）读操作方面的性能差异

RAID10可供读取有效数据的磁盘个数为4，RAID5可供读取有效数据的磁盘个数也为4个（校验信息分布在所有的盘上），所以两者的读的性能应该是基本一致的。

2）连续写方面的性能差异

在连续写操作过程，如果有写cache存在，并且算法没有问题的话，RAID5比RAID10甚至会更好一些，虽然也许并没有太大的差别。（这里要假定存储有一定大小足够的写cache，而且计算校验的cpu不会出现瓶颈）。

因为这个时候的RAID校验是在cache中完成，如4块盘的RAID5，可以先在内存中计算好校验，同时写入3个数据+1个校验。而RAID10只能同时写入2个数据+2个镜相。

如上图所示，4块盘的RAID5可以在同时间写入1、2、3到cache，并且在cache计算好校验之后，这里假定是6，同时把三个数据写到磁盘。而4块盘的RAID10不管cache是否存在，写的时候，都是同时写2个数据与2个镜相。

根据前面对缓存原理的介绍，写cache是可以缓存写操作的，等到缓存写数据积累到一定时期再写到磁盘。但是，写到磁盘阵列的过程是迟早也要发生的，所以RAID5与RAID10在连续写的情况下，从缓存到磁盘的写操作速度会有较小的区别。不过，如果不是连续性的强连续写，只要不达到磁盘的写极限，差别并不是太大。

3）离散写方面的性能差异

例如oracle 数据库每次写一个数据块的数据，如8K；由于每次写入的量不是很大，而且写入的次数非常频繁，因此联机日志看起来会像是连续写。但是因为不保证能够添满RAID5的一个条带，比如32K（保证每张盘都能写入），所以很多时候更加偏向于离散写入（写入到已存在数据的条带中）。

我们从上图看一下离散写的时候，RAID5与RAID10工作方式有什么不同。如上图：我们假定要把一个数字2变成数字4，那么对于RAID5，实际发生了4次io：先读出2与校验6，可能发生读命中然后在cache中计算新的校验写入新的数字4与新的校验8。

如上图我们可以看到：对于RAID10，同样的单个操作，最终RAID10只需要2个io，而RAID5需要4个io.

这里我忽略了RAID5在那两个读操作的时候，可能会发生读命中操作的情况。也就是说，如果需要读取的数据已经在cache中，可能是不需要4个io的。这也证明了cache对RAID5 的重要性，不仅仅是计算校验需要，而且对性能的提升尤为重要。

当然，并不是说cache对RAID10就不重要了，因为写缓冲，读命中等，都是提高速度的关键所在，只不过RAID10对cache的依赖性没有RAID5那么明显而已。

4）磁盘的IOPS对比

假定一个case，业务的iops是10000，读cache命中率是30%，读iops为60%，写iops为40%，磁盘个数为120，那么分别计算在raid5与raid10的情况下，每个磁盘的iops为多少。

raid5:

单块盘的iops = (10000*(1-0.3)*0.6 + 4 * (10000*0.4))/120

= (4200 + 16000)/120

= 168

这里的10000*(1-0.3)*0.6表示是读的iops，比例是0.6，除掉cache命中，实际只有4200个iops。

4 * (10000*0.4) 表示写的iops，因为每一个写，在raid5中，实际发生了4个io，所以写的iops为16000个

为了考虑raid5在写操作的时候，那2个读操作也可能发生命中，所以更精确的计算为：

单块盘的iops = (10000*(1-0.3)*0.6 + 2 * (10000*0.4)*(1-0.3) + 2 * (10000*0.4))/120

= (4200 + 5600 + 8000)/120

= 148

计算出来单个盘的iops为148个，基本达到磁盘极限

raid10

单块盘的iops = (10000*(1-0.3)*0.6 + 2 * (10000*0.4))/120

= (4200 + 8000)/120

= 102

可以看到，因为raid10对于一个写操作，只发生2次io，所以，同样的压力，同样的磁盘，每个盘的iops只有102个，还远远低于磁盘的极限iops。

三、小结
所以要求较高的空间利用率，对安全性要求不是特别高、大文件存储的系统采用RAID5比较好。

相反，安全性要求很高，不计成本，小数据量频繁写入的系统采用RAID10的方式比较好。



------------------------------------------------------------------------------------------------------
SSD:
SSD固态盘提供了好得多的随机读性能，单盘可达35000 IOPS (4KB或8KB等)甚至更高,并提供512MB/s或以上的读出带宽。

缓解随机读，但对随机写还是无能为力

SSD写放大：
尽管SSD的读和写都以页(page，例如4KB，8KB等)为单位，但SSD写入前需要先擦除已有内容，而擦除以块(block)为单位，一个块(block)由若干
连续的页(page)组成，大小通常在512KB~2MB左右。假如写入的页(page)内有内容，即使是写入一个字节，SSD也需要把整个block内容(512KB~2MB)先
读到内存，与要写入的内容融合，擦除整个块(block)，然后再重新写入整个块的内容，这就是SSD盘的写入放大效应。


如何把随机写转化为顺序写？LSM-Tree或Fractal Tree，消除索引的随机插入和删除带来的随机IO

（利用Fusion IO 解决主从延迟）
基于PCIe的SSD存储产品，例如Fusion IO、Virident等的PCIe的SSD产品，通过采用PCI板上内存、板上电容(防止数据丢失)等
提升固态存储设备的读写性能，尤其是随机写性能
