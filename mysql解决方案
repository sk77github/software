mysql扩容：

mysql在线扩容和缩容一般涉及到的内容，主要包括三个方面，
1.在线也就意味着需要把增量的数据重新分布到新的拓扑结构中,我们一般称做增量复制，
2.原有的数据需要一条不漏的扫出来重新分布到新的拓扑结构中,这个一般叫做全量复制，
3.全量做完，增量正在同步，把应用的数据路由拓扑切到新的路由拓扑上来，并且做到无数据丢失，这个我们叫做停写切换。

做好这三个方面的工作，能够达到的效果就是应用在最后切换数据分布拓扑的时刻，只要停写非常短的时间(秒级别)就能够做到无数据丢失的扩容和缩容。
增量同步一般有2种方式，一种是应用端或者数据库前端做trigger,记录变更数据的特征值log(比如pk,sharding key)，然后异步复制到新的拓扑结构中。另外一种方式是通过分析mysql的binlog再进行不同数据拓扑的复制。两者本质上来说应该是一样的，后者可能更加简便，并且对应用无侵入，前者虽然也能够做到，实际实现或者推广和操作上都有不少阻力，最起码解析binlog方式是mysql一上去，更新的log已经天然存在与binlog中了。 
增量同步的两种方式如果要考虑到同步的可伸缩性(也就是多台机器可以同时消费相同的变更日志)，需要在原数据中添加数据的版本信息防止更新乱序，或者通过唯一键进行复制机器的sharding,也就是不同进程(线程)同时消费相同的更新日志，必须让同一条记录的更新落在同一个线程里面，如果还需要保证复制的事务，那么实现会非常复杂，一般不会去支持多线程下复制的事务。 
全量复制，也就是扫描需要复制的表的数据进行重新分布，主要存在的问题是复制速度和对数据库的写入压力的矛盾，其实能够做到整个拓扑连数据库都全部换掉，来达到对正在使用数据库的0影响，这个是一种可行的方案，另外是分时段调整复制线程数，一般单线程复制对于数据库的影响不会很大，在凌晨再转换成多线程方式达到提速的目标。 
扩容或者缩容在最后阶段如何切换，这个涉及到的问题主要是如何避免新更新进来以至于增量没完没了，方式有很多，最简单的方法就是停掉应用，一般时间只有几分钟是可以接受的。另外一种是逻辑停写，因为我们迁移的时候是有一个规则去重新散列数据，也就是如果新的规则和旧的规则两者算出来的结果不一致，那么这个数据就是需要被迁移的，如果在停写的时刻，向前端抛错即可。逻辑停写最大的好处就是避免PE的介入，并且配合动态的数据路由数据推送，可以完全避免重新发布达到扩容或者缩容，这个就是真正的在线扩容，停写不可避免（等待延迟的增量同步完成），但是不影响读。 
数据扩容或者缩容，我们觉得不应该排入业务的开发日程中，而是由数据管理团队对应用透明地进行这种操作，最后介入的人员只是DBA而已。但是不像一些nosql一样按容量或者完全透明的split,数据库的sharding还是按照应用的数据特性（pk,user_id,gmt_create等等不同字段，自选策略）进行sharding,应用知道他们的某条数据具体存在哪个机器哪张表上，这个无论对于开发还是测试或者DBA都是一件不错的事情。 

http://www.csdn.net/article/2015-06-02/2824824
自动扩容机制

目前，针对MySQL的扩容，一般有下面两种策略。

垂直扩容。一般通过升级硬件来实现，比如更换更好的CPU，将传统的sas盘换成FusionIO卡这类，然后针对新硬件调整好参数，在硬件结构变化比较大的时候，性能甚至能达到上十倍的提升。但垂直扩容有比较大的局限，就是这种模式随着业务的突增还是比较容易达到瓶颈，特别是面对互联网海量用户的时候，所以在互联网应用场景下，一般仅将垂直扩容当做一个辅助的手段。
水平扩容。常用的有2种方法，一是不同的库或者表部署到不同的实例，二是一张表需要根据某个字段拆分到不同的字表中（数据分片），这种策略在互联网系统中非常常见，很多系统会将这2种水平扩容的方法结合起来使用；
通过上述2种扩容方法的比较，为了应对海量扩展的需求，应该是重点选用水平扩容的方法。但水平扩容的实现一般对业务是有感知的，比如采用什么规则来拆表，拆开的表放到哪些节点，如果某个子表还有瓶颈应该怎么扩容，扩容是否还需要业务配合等等这些事情如果全部交给业务会比较繁琐，因此这些需求应该尽量全部交给TDSQL自身来完成，对业务完全透明。

分表逻辑

在TDSQL中，每个表（逻辑表）可能会拆分成多个子表（建表的时候通过在建表语句中嵌入注释的方式提供一个shard字段名，最多会拆分出1W个子表），每个子表在MySQL上都是一个真实的物理表，这里称为一个shard，因此一张表的数据可能会按这样的方式分布在多个Set中，如图2所示



图2 TDSQL的逻辑表

每个SQL请求到达网关之后，网关会做词法和语法解析，重点会解析出shard字段，如果带了shard字段就可以直接查询路由表并发送到某个具体的set中。计费的OLTP类业务99%的请求都会带上shard字段；如果某笔请求没有shard字段，查询路由之后会将请求发送到所有的shard对应的set中，并对所有返回的结果做一些聚合运算。

扩容流程

上面描述了shard的方式，但是这样的shard结构不是固定不变的，当Scheduler检测到某个set，某个表的CPU、磁盘超过阈值之后就会启动扩容流程。

这里描述下具体的扩容流程。

扩容过程中一般都要尽量避免影响业务，目前来看存在2种比较成熟的策略。

策略1先切后搬：先修改路由，将需要迁走的数据的请求直接发送到新set，在新set交易过程中如发现本地的数据不存在，则去原set拉取数据，然后再通过一些离线的策略将要迁移的数据全量再搬迁一次，HOID平台就是采用这样的策略。

策略2先搬后切：让请求继续在原set交易，扩容程序首先记录一个binlog位置点，并将源set中符合迁移条件的数据全部迁移出去，最后再将搬迁过程中新增的binlog追完，最后修改路由规则，将请求发送到新set。

综合来看，策略1最大的优点是假如是因为压力大做的迁移，可能很快就能将部分请求发送新set了，实现对原set的压力分担；策略2实现上在最后的追路由阶段需要更多的精细化控制，实现会稍微复杂点，但策略2有个非常大的好处就是扩容过程中回滚非常方便，如有异常直接干掉扩容任务即可。

对于TDSQL这类数据库业务系统来说，策略1实现会非常麻烦，因为请求到达新set之后可能需要去源set拉取数据，这个需要对MySQL本身进行修改；另外假如一个批量更新的update操作，可能要往新老set都发送一次请求，比较复杂，所以最终选择了策略2。策略2会有更大的通用性，开发模式基本上可以统一到所有类似的系统。

下面描述采用策略2具体的扩容流程。假如要将Set1中的t_shard_1的数据迁移一半到Set4中的t_shard_4(1667-3333)。



图3 策略2的扩容流程

Scheduler首先在Set4中创建好表t_shard_4。

后将扩容任务下发到Set1中的agent模块，agent检测到扩容任务之后会采用mysqldump+where条件的方式将t_shard_1中shard号段为1667-3333的记录导出来并通过管道用并行的方式插入到Set4（不会在本地存文件，避免引起过多的IO），用mysqldump导出镜像的时候会有一个binlog位置。

从mysqldump记录的binlog位置开始读取binlog并插入到到Set4，追到所有binlog文件末尾的时候（这需要一个循环，每次循环记录从开始追binlog截止到追到文件结尾消耗的时间，必须保证追单次循环要在几秒之内完成，避免遗留的binlog太多导致最后一次追binlog消耗太多的时间，从而影响业务过久），对原来的表t_shard_1重命名t_shard_5，此时针对这个表不会再有新请求，若还有请求过来都会失败，然后再追一次binlog到文件结尾（因为上面的循环保证了追binlog不会太耗时间了，所以此次会快速完成），然后上报状态到ZooKeeper，表明扩容任务完成。

Scheduler收到扩容完成的信息之后会修改路由表，最后由网关拉取到新路由完成整体的扩容；从表重命名开始到网关拉取到新路由，这段时间这个原始shard不可用，从我们测试结果来看这个不可用的时间是200毫秒左右；如果某个网关异常，拉取不到新路由，继续访问老表t_shard_1会一直失败，这样就可以保证数据的一致性。
---------------------------------------------------------------------------------------------------------------------

facebook：
mysql5.6的多线程复制来解决write scale
用半同步来解决data loss，
用gtid、binlog server、mha来解决高可用问题



Fabric


Galera Cluster的实现方案有三种：Galera Cluster for MySQL、Percona XtraDB Cluster、MariaDB Galera Cluster。



HA：
1，replication
主宕机，已有数据不丢失：
通过redo log，xa 事务，gelera，

2，moniter：
主宕机，从无缝切换为主，新写数据不丢失
首先，程序（或者某个组件）怎么第一时间知道主宕机了，如何做监控发现。（mha，keepalived（VIP）或者heartbeat，zookeeper）
从如何保证与主有一致性的数据，主从不延时，且无数据丢失(同步复制，半同步复制）
这样，相同的数据有了，也第一时间发现了主宕机，那么如何将应用写入切换到新主，也就是如何把从提升为主

3，failover
keepalived（上层应用与mysql复制集群的通信切换）
mha（mysql集群内部复制拓扑的切换）
gtid

使用keepalived的目的就是在MHA检测到master挂掉的时候,调用shutdown_script关掉keepalived进程,从而使虚拟IP（VIP）移动到新的master上面去。

参考资料：
http://yoshinorimatsunobu.blogspot.jp/2014/04/semi-synchronous-replication-at-facebook.html

市面现有方案：
mha	
keeplived       实现虚拟IP
mmm　　	        主备通过VIP共享ip 
heartbeat+ drbd	主备通过VIP共享ip 　　　　
percona galera cluster

 





中间件：
vitess
https://github.com/twitter/gizzard
kingshard
mixer
atlas



